{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2.4.Deep_learning_for_computer_vision.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"-QqpoZedlovX","colab_type":"text"},"cell_type":"markdown","source":["#2.4 Convolutional Neural Networks in Keras\n","Convolutional Neural Network (CNN) are very popular architecture in the field of computervision.  This deep network architecture performs very well in areas such as image classification, image recognition and object detection. The architecture is designed to take advantage of the 2D structure of the input data such as image or a speech signal.\n","\n","In this tutorial we will use a very well known dataset called** Fashion Mnist** .Normally , **MNIST**-which consist ten different classes of hand written digits- is the starting point for many initial CNN learners. But we have chosen this dataset,which consist of ten different classes fashion items like clothes, shoes, bags etc, as MNIST has become very easy and over used dataset.  Some of the instances of the data are shown in figure given below:\n","\n","\n","![ Fig: Fashion MNIST ](https://raw.githubusercontent.com/zalandoresearch/fashion-mnist/master/doc/img/fashion-mnist-sprite.png)\n","\n","*Figure: Samples of Fashion MNIST dataset*"]},{"metadata":{"id":"2ZzTZtgR7XhH","colab_type":"text"},"cell_type":"markdown","source":["##2.4.1 Data Preparation\n","We can start by downloading the data set . The data set is available at keras server. It consists of 60000 grayscale images of size 28*28 as training set and  10000 test images with total 10 classes. The data is stored as numpy array so we can check this detail using shape property of the array.\n","\n"]},{"metadata":{"id":"ZYFYnywC7WdV","colab_type":"code","outputId":"f0872da1-11f8-4f45-a3e8-b807f9c57b0b","executionInfo":{"status":"ok","timestamp":1548429253544,"user_tz":-345,"elapsed":1684,"user":{"displayName":"Bishal Lakha","photoUrl":"","userId":"01645996282289086361"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"cell_type":"code","source":["from keras.datasets import fashion_mnist\n","\n","(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n","\n","print(\"Training data detail:\", x_train.shape)\n","print(\"Test data detail:\", x_test.shape)\n"],"execution_count":38,"outputs":[{"output_type":"stream","text":["Training data detail: (60000, 28, 28)\n","Test data detail: (10000, 28, 28)\n"],"name":"stdout"}]},{"metadata":{"id":"NmXPH5HH9jv_","colab_type":"text"},"cell_type":"markdown","source":["We can also inspect each element of our data set. Here we will check 11th image of our training data set. "]},{"metadata":{"id":"Xn8WKAwiXSjA","colab_type":"code","outputId":"4392157e-502a-4230-9315-1abcad0e2fd5","executionInfo":{"status":"ok","timestamp":1548429255318,"user_tz":-345,"elapsed":1862,"user":{"displayName":"Bishal Lakha","photoUrl":"","userId":"01645996282289086361"}},"colab":{"base_uri":"https://localhost:8080/","height":282}},"cell_type":"code","source":["import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","plt.imshow(x_train[10])"],"execution_count":39,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.image.AxesImage at 0x7fec24b20a58>"]},"metadata":{"tags":[]},"execution_count":39},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAPoAAAD4CAYAAADFJPs2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"M35KzsdXLOqw","colab_type":"text"},"cell_type":"markdown","source":["Before we can use our data , we need to convert our 3 dimensional data to 4 dimensional data to make it compatible with Keras. We achieve this by using numpy reshape function. We perform this action to both train and test data."]},{"metadata":{"id":"myVuvzOHBuQ9","colab_type":"code","colab":{}},"cell_type":"code","source":["img_rows, img_cols = 28, 28\n","\n","x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n","x_test = x_test.reshape(x_test.shape[0],img_rows, img_cols,1)\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"PFE-5Jm8PNFF","colab_type":"text"},"cell_type":"markdown","source":["We will also have to normalize our data to get better result. Since we are using grayscale image , we can normalize our image by dividing it by 255. This restricts the pixel value from 0 to 1 resulting in better performance."]},{"metadata":{"id":"4WkZ0UJrJiqm","colab_type":"code","outputId":"41018dca-3cd8-41d7-dea0-b390a7139358","executionInfo":{"status":"ok","timestamp":1548421786074,"user_tz":-345,"elapsed":17464,"user":{"displayName":"Bishal Lakha","photoUrl":"","userId":"01645996282289086361"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"cell_type":"code","source":["x_train = x_train.astype('float32')\n","x_test = x_test.astype('float32')\n","x_train /= 255\n","x_test /= 255\n","print('x_train shape:', x_train.shape)\n","print(x_train.shape[0], 'train samples')\n","print(x_test.shape[0], 'test samples')"],"execution_count":4,"outputs":[{"output_type":"stream","text":["x_train shape: (60000, 28, 28, 1)\n","60000 train samples\n","10000 test samples\n"],"name":"stdout"}]},{"metadata":{"id":"jCXYjLBwHDmu","colab_type":"text"},"cell_type":"markdown","source":["We will also need to convert our target values into one hot encoding like we did in our earlier chapter."]},{"metadata":{"id":"ezEbuPth3_rb","colab_type":"code","colab":{}},"cell_type":"code","source":["import keras \n","num_classes = 10\n","\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"t0vOaKc-L8MW","colab_type":"text"},"cell_type":"markdown","source":["##2.4.2 Model Building\n","After preparing our data , we now need to build our cnn architecture. A CNN network is composed of different layers like convolutional layer, pooling layer, dense layer and other as shown in figure. We will import this from keras layers class. We will also need to flatten the results of convolutional layer before feeding it to dense layer. For this purpose we will use Flatten class . We will also use Dropout for regularization.\n","\n","![Figure CNN layer](https://www.jeremyjordan.me/content/images/2018/04/vgg16.png)\n","\n","*Figure : Convolutional Neural Network Architecture*"]},{"metadata":{"id":"F5226hUQ3yGx","colab_type":"code","colab":{}},"cell_type":"code","source":["import keras\n","from keras.models import Sequential\n","from keras.layers import Dense, Dropout, Flatten\n","from keras.layers import Conv2D, MaxPooling2D\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"DKSL8PQUJg41","colab_type":"text"},"cell_type":"markdown","source":["We will start by creating an instance of Sequential class and add required layers step by step."]},{"metadata":{"id":"5RwKEI9kKRQF","colab_type":"code","colab":{}},"cell_type":"code","source":["  model = Sequential()\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"n8xLkWeSKVSz","colab_type":"text"},"cell_type":"markdown","source":["We will now add our first convolutional layer which has input shape of our image. It will use 32 layers of 3*3 kernels. We will the use** relu**  as our activation function."]},{"metadata":{"id":"OJEebs1fKpWG","colab_type":"code","colab":{}},"cell_type":"code","source":["input_shape = (img_rows, img_cols,1)\n","model.add(Conv2D(32, kernel_size=(3, 3),\n","                   activation='relu',\n","                   input_shape=input_shape))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"p-v6gCw9LbUf","colab_type":"text"},"cell_type":"markdown","source":["We wil add another convolutional layer which uses 64 layers of 3*3 kernel . We will use the same activation function as the previous layer"]},{"metadata":{"id":"CxfpKh3nL3vu","colab_type":"code","colab":{}},"cell_type":"code","source":["  model.add(Conv2D(64, (3, 3), activation='relu'))\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"wI15C4r6MPmJ","colab_type":"text"},"cell_type":"markdown","source":["We will now use max pooling layer of size 2*2 to reduce spatial size of our feature map.\n","We will also use drop out of 25% which mean 25% of neuron used upto that points are randomly dropped. This prevents our CNN model from overfitting."]},{"metadata":{"id":"Fme3pHFSMP19","colab_type":"code","colab":{}},"cell_type":"code","source":["model.add(MaxPooling2D(pool_size=(2, 2)))\n","model.add(Dropout(0.25))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"0pvAytFSNITe","colab_type":"text"},"cell_type":"markdown","source":["The dense layer takes input as a 1D vector but the output from convolutional layer ar multi-dimensional. So we flatten our feature map provided from our convolutional layer. We will feed this flattened layer to a dense layer of 128 neurons.The layer will be followed by **relu** activation function We will also add a dropout after the dense layer.\n","\n"]},{"metadata":{"id":"s1ZfrCycNQj5","colab_type":"code","colab":{}},"cell_type":"code","source":["model.add(Flatten())\n","model.add(Dense(128, activation='relu'))\n","model.add(Dropout(0.5))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"i3EadP4sSJhL","colab_type":"text"},"cell_type":"markdown","source":["We will then add our final layer which is also a dense layer. But we should be careful about choosing the total number of neurons in this layer. We need to make the number of neurons equals to to total number of classes as we will use softmax function which will predict the probability of given example falling into particular class.\n","\n"]},{"metadata":{"id":"sK2MHm41SIgS","colab_type":"code","colab":{}},"cell_type":"code","source":["model.add(Dense(num_classes, activation='softmax'))\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"jDPGvIUPS1cv","colab_type":"text"},"cell_type":"markdown","source":["We have successfully created our CNN architecture which consists of two convolutional layers and two dense layers. In keras, as we have done in earlier chapter, we  have to complile our model  before training it. While compiling , we will set categorical cross entropy as our loss function Adadelta as our optimizer and we will measure performance of our model using **accuracy **matric."]},{"metadata":{"id":"MF4lEJGES0VP","colab_type":"code","colab":{}},"cell_type":"code","source":["model.compile(loss=keras.losses.categorical_crossentropy,\n","              optimizer=keras.optimizers.Adadelta(),\n","              metrics=['accuracy'])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ISOdGF7Sel8B","colab_type":"text"},"cell_type":"markdown","source":["For simplification ( and also for reusing code in different part of our chapter) we will create a function called CNN which will build a model we described above."]},{"metadata":{"id":"UB0l7ngu4iLS","colab_type":"code","colab":{}},"cell_type":"code","source":["num_classes = 10\n","\n","def CNN():\n","\n","  model = Sequential()\n","  model.add(Conv2D(32, kernel_size=(3, 3),\n","                   activation='relu',\n","                   input_shape=input_shape))\n","  model.add(Conv2D(64, (3, 3), activation='relu'))\n","  model.add(MaxPooling2D(pool_size=(2, 2)))\n","  model.add(Dropout(0.25))\n","  model.add(Flatten())\n","  model.add(Dense(128, activation='relu'))\n","  model.add(Dropout(0.5))\n","  model.add(Dense(num_classes, activation='softmax'))\n","  \n","  model.compile(loss=keras.losses.categorical_crossentropy,\n","              optimizer=keras.optimizers.Adadelta(),\n","              metrics=['accuracy'])\n","  \n","  return model"],"execution_count":0,"outputs":[]},{"metadata":{"id":"UFW3QG1EnSNS","colab_type":"text"},"cell_type":"markdown","source":["We can see the output size of different layers of our model using model summary fuction. We can see that our model have about 2 million parameters."]},{"metadata":{"id":"CH6H_F16e54F","colab_type":"code","outputId":"55b0c06e-78c5-482b-fd64-8d3331377988","executionInfo":{"status":"ok","timestamp":1548422135615,"user_tz":-345,"elapsed":1444,"user":{"displayName":"Bishal Lakha","photoUrl":"","userId":"01645996282289086361"}},"colab":{"base_uri":"https://localhost:8080/","height":425}},"cell_type":"code","source":["model_no_aug = CNN()\n","print(model_no_aug.summary())"],"execution_count":30,"outputs":[{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv2d_5 (Conv2D)            (None, 26, 26, 32)        320       \n","_________________________________________________________________\n","conv2d_6 (Conv2D)            (None, 24, 24, 64)        18496     \n","_________________________________________________________________\n","max_pooling2d_3 (MaxPooling2 (None, 12, 12, 64)        0         \n","_________________________________________________________________\n","dropout_5 (Dropout)          (None, 12, 12, 64)        0         \n","_________________________________________________________________\n","flatten_3 (Flatten)          (None, 9216)              0         \n","_________________________________________________________________\n","dense_5 (Dense)              (None, 128)               1179776   \n","_________________________________________________________________\n","dropout_6 (Dropout)          (None, 128)               0         \n","_________________________________________________________________\n","dense_6 (Dense)              (None, 10)                1290      \n","=================================================================\n","Total params: 1,199,882\n","Trainable params: 1,199,882\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n"],"name":"stdout"}]},{"metadata":{"id":"0SToTRO7MBGA","colab_type":"text"},"cell_type":"markdown","source":["##2.4.3 Training and Testing"]},{"metadata":{"id":"e-j3S2BYMhg4","colab_type":"text"},"cell_type":"markdown","source":["###2.4.3.1 Training without data augmentation\n","In this chapter we will firstly explore to train our model only in our data. Then we will explore how it will behave with data augmentation."]},{"metadata":{"id":"Qw8QFE-kL_-M","colab_type":"code","colab":{}},"cell_type":"code","source":["batch_size = 32\n","epochs = 12"],"execution_count":0,"outputs":[]},{"metadata":{"id":"cZiwbJoB9g5v","colab_type":"code","outputId":"7de07447-9dc3-45b5-a847-acbd5a3d6268","executionInfo":{"status":"ok","timestamp":1548422394973,"user_tz":-345,"elapsed":243749,"user":{"displayName":"Bishal Lakha","photoUrl":"","userId":"01645996282289086361"}},"colab":{"base_uri":"https://localhost:8080/","height":479}},"cell_type":"code","source":["model_no_aug.fit(x_train, y_train,validation_split=0.1,\n","          batch_size=batch_size,\n","          epochs=epochs,\n","          verbose=1)\n"],"execution_count":32,"outputs":[{"output_type":"stream","text":["Train on 54000 samples, validate on 6000 samples\n","Epoch 1/12\n","54000/54000 [==============================] - 19s 354us/step - loss: 0.4866 - acc: 0.8283 - val_loss: 0.3185 - val_acc: 0.8773\n","Epoch 2/12\n","54000/54000 [==============================] - 19s 344us/step - loss: 0.3207 - acc: 0.8879 - val_loss: 0.2533 - val_acc: 0.9105\n","Epoch 3/12\n","54000/54000 [==============================] - 19s 343us/step - loss: 0.2735 - acc: 0.9028 - val_loss: 0.2382 - val_acc: 0.9140\n","Epoch 4/12\n","54000/54000 [==============================] - 18s 342us/step - loss: 0.2506 - acc: 0.9126 - val_loss: 0.2464 - val_acc: 0.9102\n","Epoch 5/12\n","54000/54000 [==============================] - 18s 341us/step - loss: 0.2340 - acc: 0.9192 - val_loss: 0.2508 - val_acc: 0.9178\n","Epoch 6/12\n","54000/54000 [==============================] - 18s 342us/step - loss: 0.2231 - acc: 0.9229 - val_loss: 0.2168 - val_acc: 0.9215\n","Epoch 7/12\n","54000/54000 [==============================] - 18s 342us/step - loss: 0.2130 - acc: 0.9268 - val_loss: 0.2121 - val_acc: 0.9217\n","Epoch 8/12\n","54000/54000 [==============================] - 18s 341us/step - loss: 0.2082 - acc: 0.9292 - val_loss: 0.2415 - val_acc: 0.9183\n","Epoch 9/12\n","54000/54000 [==============================] - 19s 343us/step - loss: 0.1997 - acc: 0.9315 - val_loss: 0.2339 - val_acc: 0.9175\n","Epoch 10/12\n","54000/54000 [==============================] - 18s 342us/step - loss: 0.1959 - acc: 0.9335 - val_loss: 0.2357 - val_acc: 0.9202\n","Epoch 11/12\n","54000/54000 [==============================] - 18s 342us/step - loss: 0.1910 - acc: 0.9352 - val_loss: 0.2071 - val_acc: 0.9242\n","Epoch 12/12\n","54000/54000 [==============================] - 18s 342us/step - loss: 0.1818 - acc: 0.9387 - val_loss: 0.2175 - val_acc: 0.9232\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7fec2b4e9c18>"]},"metadata":{"tags":[]},"execution_count":32}]},{"metadata":{"id":"g1iqIiamw7Cq","colab_type":"code","outputId":"e7293a68-2419-494f-9dca-e19c4c709877","executionInfo":{"status":"ok","timestamp":1548422484276,"user_tz":-345,"elapsed":2552,"user":{"displayName":"Bishal Lakha","photoUrl":"","userId":"01645996282289086361"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"cell_type":"code","source":["score = model_no_aug.evaluate(x_test, y_test, verbose=0)\n","print('Test loss:', score[0])\n","print('Test accuracy:', score[1])"],"execution_count":33,"outputs":[{"output_type":"stream","text":["Test loss: 0.24024140211939812\n","Test accuracy: 0.9191\n"],"name":"stdout"}]},{"metadata":{"id":"HSLZrYtqeWDl","colab_type":"text"},"cell_type":"markdown","source":["###2.4.3.2 Training with data augmentation"]},{"metadata":{"id":"o_FmJ7WPv0jY","colab_type":"text"},"cell_type":"markdown","source":["Data augmentation comes handy when we have very few data or when our model is over-fitting. To mimic the scenario of lack of data we will use the first 1000 images and train our model as previously."]},{"metadata":{"id":"Fzc0LZ2wixuS","colab_type":"code","outputId":"1a2b418e-bb18-4f0c-bb81-94de255c21b9","executionInfo":{"status":"ok","timestamp":1548211038521,"user_tz":-345,"elapsed":7867,"user":{"displayName":"Bishal Lakha","photoUrl":"","userId":"01645996282289086361"}},"colab":{"base_uri":"https://localhost:8080/","height":476}},"cell_type":"code","source":["x_train = x_train[:999]\n","y_train = y_train[:999]\n","\n","model = CNN()\n","\n","model.fit(x_train, y_train,validation_split=0.1,\n","          batch_size=batch_size,\n","          epochs=epochs,\n","          verbose=1)\n","score = model.evaluate(x_test, y_test, verbose=0)\n","print('Test loss:', score[0])\n","print('Test accuracy:', score[1])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Train on 899 samples, validate on 100 samples\n","Epoch 1/12\n","899/899 [==============================] - 1s 1ms/step - loss: 1.7337 - acc: 0.4149 - val_loss: 1.9373 - val_acc: 0.3900\n","Epoch 2/12\n","899/899 [==============================] - 0s 353us/step - loss: 0.9652 - acc: 0.6574 - val_loss: 0.8540 - val_acc: 0.7200\n","Epoch 3/12\n","899/899 [==============================] - 0s 362us/step - loss: 0.7488 - acc: 0.7564 - val_loss: 0.6538 - val_acc: 0.7900\n","Epoch 4/12\n","899/899 [==============================] - 0s 362us/step - loss: 0.6105 - acc: 0.7831 - val_loss: 0.6577 - val_acc: 0.7800\n","Epoch 5/12\n","899/899 [==============================] - 0s 357us/step - loss: 0.5668 - acc: 0.8209 - val_loss: 0.4960 - val_acc: 0.8000\n","Epoch 6/12\n","899/899 [==============================] - 0s 359us/step - loss: 0.4937 - acc: 0.8220 - val_loss: 0.5983 - val_acc: 0.7900\n","Epoch 7/12\n","899/899 [==============================] - 0s 362us/step - loss: 0.4749 - acc: 0.8398 - val_loss: 0.4777 - val_acc: 0.8600\n","Epoch 8/12\n","899/899 [==============================] - 0s 356us/step - loss: 0.4072 - acc: 0.8554 - val_loss: 0.4928 - val_acc: 0.7900\n","Epoch 9/12\n","899/899 [==============================] - 0s 350us/step - loss: 0.3638 - acc: 0.8643 - val_loss: 0.5615 - val_acc: 0.8000\n","Epoch 10/12\n","899/899 [==============================] - 0s 353us/step - loss: 0.3401 - acc: 0.8710 - val_loss: 0.5310 - val_acc: 0.7800\n","Epoch 11/12\n","899/899 [==============================] - 0s 362us/step - loss: 0.2927 - acc: 0.8988 - val_loss: 0.5166 - val_acc: 0.8400\n","Epoch 12/12\n","899/899 [==============================] - 0s 354us/step - loss: 0.2662 - acc: 0.8999 - val_loss: 0.5382 - val_acc: 0.8300\n","Test loss: 0.6214321170330047\n","Test accuracy: 0.7958\n"],"name":"stdout"}]},{"metadata":{"id":"aBWBBiC8skOc","colab_type":"text"},"cell_type":"markdown","source":["With lack of data we clearly see how our model performance has degraded. While we have achieved accuracy of above 91% with 60000 thaousand train data, our model performance has now come down to about 79% accuracy. The case may be even worse with other more complicated task."]},{"metadata":{"id":"C3Yzex_QteIh","colab_type":"text"},"cell_type":"markdown","source":["###2.4.3.2 Training with data augmentation\n","Augmenting image data is very easy in Keras. We will use ImageDataGenerator class of keras to carryout different image augmentation. Here we have instatntiate an ImageDataGenerator object which will carry out image augmentation methods like rotation, image shifting and flipping. "]},{"metadata":{"id":"rsOStAc4sZ55","colab_type":"code","colab":{}},"cell_type":"code","source":["from keras.preprocessing.image import ImageDataGenerator\n","\n","x_train = x_train[:999]\n","y_train = y_train[:999]\n","epochs = 12\n","\n","model = CNN()\n","image_gen = ImageDataGenerator(\n","    rotation_range=15,\n","    width_shift_range=.15,\n","    height_shift_range=.15,\n","    horizontal_flip=True)\n","\n","#training the image preprocessing\n","image_gen.fit(x_train, augment=True)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"bSsZO-WruqYH","colab_type":"text"},"cell_type":"markdown","source":["Unlike our previous approach, when we have directly fed our image to our model , we will now have to feed our ImageDataGenerator object which will produce new set of images with different combination of data augmentation while training . To make this happen the fit function that we previously used won't be helpful. We ,thus, use a function called fit_generator. With the data augmentation technique implemented , we can clearly see our model performing better than than without using it."]},{"metadata":{"id":"_1j0-Km8eTUy","colab_type":"code","outputId":"8bf46661-1f08-44b3-ec02-4795f420e515","executionInfo":{"status":"ok","timestamp":1548211261686,"user_tz":-345,"elapsed":197789,"user":{"displayName":"Bishal Lakha","photoUrl":"","userId":"01645996282289086361"}},"colab":{"base_uri":"https://localhost:8080/","height":459}},"cell_type":"code","source":["model.fit_generator(image_gen.flow(x_train, y_train, batch_size=batch_size),\n","          steps_per_epoch=  x_train.shape[0],\n","          epochs=epochs,\n","          verbose=1)\n","\n","score = model.evaluate(x_test, y_test, verbose=0)\n","print('Test loss:', score[0])\n","print('Test accuracy:', score[1])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch 1/12\n","999/999 [==============================] - 17s 17ms/step - loss: 0.9379 - acc: 0.6534\n","Epoch 2/12\n","999/999 [==============================] - 16s 16ms/step - loss: 0.6399 - acc: 0.7671\n","Epoch 3/12\n","999/999 [==============================] - 16s 16ms/step - loss: 0.5483 - acc: 0.8010\n","Epoch 4/12\n","999/999 [==============================] - 16s 16ms/step - loss: 0.4957 - acc: 0.8194\n","Epoch 5/12\n","999/999 [==============================] - 16s 16ms/step - loss: 0.4490 - acc: 0.8356\n","Epoch 6/12\n","999/999 [==============================] - 16s 16ms/step - loss: 0.4189 - acc: 0.8475\n","Epoch 7/12\n","999/999 [==============================] - 16s 16ms/step - loss: 0.3889 - acc: 0.8599\n","Epoch 8/12\n","999/999 [==============================] - 16s 16ms/step - loss: 0.3751 - acc: 0.8663\n","Epoch 9/12\n","999/999 [==============================] - 16s 16ms/step - loss: 0.3579 - acc: 0.8719\n","Epoch 10/12\n","999/999 [==============================] - 16s 16ms/step - loss: 0.3430 - acc: 0.8771\n","Epoch 11/12\n","999/999 [==============================] - 16s 16ms/step - loss: 0.3362 - acc: 0.8798\n","Epoch 12/12\n","999/999 [==============================] - 16s 16ms/step - loss: 0.3249 - acc: 0.8833\n","Test loss: 0.6799577817440033\n","Test accuracy: 0.8291\n"],"name":"stdout"}]},{"metadata":{"id":"OOssV_NvMQmw","colab_type":"text"},"cell_type":"markdown","source":["##2.4.4 What did our model learn?"]},{"metadata":{"id":"UP2WRP3qg3jy","colab_type":"text"},"cell_type":"markdown","source":["\"Visualizing intermediate activations consists of displaying the feature maps that are output by various convolution and pooling layers in a network, given a certain input (the output of a layer is often called its activation, the output of the activation function). This gives a view into how an input is decomposed into the different filters learned by the network. Each channel encodes relatively independent features, so the proper way to visualize these feature maps is by independently plotting the contents of every channel as a 2D image\"  says Fran√ßois Chollet, author of keras. \n","Knowing how different layer are learning from our data is very helpful to get insight on how deep learning works. It help us understand how these successive layers are transforming their input.. So we will be using our **model_no_aug** model -,which was trained with all data with out data augmentation, to visualize how its layers are learning from our images. We will start by making list of all the layers of our model."]},{"metadata":{"id":"uHbPqfzD9uaV","colab_type":"code","colab":{}},"cell_type":"code","source":["layer_outputs = [layer.output for layer in model_no_aug.layers] \n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"evXl1nq4imUI","colab_type":"text"},"cell_type":"markdown","source":["We will now create activation models from the layers of our model using keras class Model. We feed it image input while it returns the values of layer of activation. Since we have multiple layers, we create a multi-output model which can take many input and many output. "]},{"metadata":{"id":"plqD6gVK6PXK","colab_type":"code","colab":{}},"cell_type":"code","source":["from keras import models\n","\n","activation_model = models.Model(inputs=model_no_aug.input, outputs=layer_outputs)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"WgRTf04NmOry","colab_type":"text"},"cell_type":"markdown","source":["Now will use second image from our training dataset to find out how our layers are behaving. Our second image is a t-shirt which we can see below."]},{"metadata":{"id":"eCUNmrRmk5Lq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":284},"outputId":"51875b86-947a-4a0f-9e6d-ec5e228d5654","executionInfo":{"status":"ok","timestamp":1548429339471,"user_tz":-345,"elapsed":1650,"user":{"displayName":"Bishal Lakha","photoUrl":"","userId":"01645996282289086361"}}},"cell_type":"code","source":["plt.imshow(x_train[1])"],"execution_count":42,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.image.AxesImage at 0x7fec24a24dd8>"]},"metadata":{"tags":[]},"execution_count":42},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAPoAAAD4CAYAAADFJPs2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"W-BY9emPmzd9","colab_type":"text"},"cell_type":"markdown","source":["We will now use our activation model to make prediction on our image. As this is a multi-modal layer , it gives result for all of our layers. We will what our first convolutional layer has learned."]},{"metadata":{"id":"c3_JyBst7Fos","colab_type":"code","colab":{}},"cell_type":"code","source":["x = x_train[1].reshape(1,28,28,1)\n","\n","activations = activation_model.predict(x) \n","\n","first_layer_activation = activations[0]\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"eQnsvlHxn754","colab_type":"text"},"cell_type":"markdown","source":["It seems  like our first layer has sucessfuly encode a virtical edge detector. In the plot below, we can clearly see that the vertical lines of our t-sirt is being activated (observed from bright color)"]},{"metadata":{"id":"vCQKgqqy7u2n","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":292},"outputId":"f8c2dd98-80e8-445d-b906-6c229c5457fd","executionInfo":{"status":"ok","timestamp":1548422510491,"user_tz":-345,"elapsed":6369,"user":{"displayName":"Bishal Lakha","photoUrl":"","userId":"01645996282289086361"}}},"cell_type":"code","source":["plt.matshow(first_layer_activation[0, :, :, 4], cmap='viridis')\n"],"execution_count":37,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.image.AxesImage at 0x7fec24bdce48>"]},"metadata":{"tags":[]},"execution_count":37},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAQEAAAEACAYAAACzsMNYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxyRA5DkmASLPMQkQee7/AVZizFaWNoELAAAAAElFTkSuQmCC\n","text/plain":["<Figure size 288x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"ABv077uzoXjB","colab_type":"text"},"cell_type":"markdown","source":["You can similarly inspect other layers and find out what our layers has learned. While doing so, you will find out that different layers learn different things and as we go higher , what layer learns become more abstract and less interpretable. Which means our model starts by learning simple feature and eands up learning complex features which it later uses to make predictions."]},{"metadata":{"id":"izwQLkOp9A8q","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}